{"cells":[{"metadata":{},"cell_type":"markdown","source":"***\n\n***\n# Logistic Regression Project (Predict Ad click)\n\nIn this notebook we will use `Logistic Regression` to indicating whether or not a particular internet user clicked on an Advertisement. We will try to create a model that will predict whether or not they will click on an ad based off the features of that user.\n\nThis data set contains the following features:\n\n* '`Daily Time Spent on Site`': consumer time on site in minutes\n* '`Age`': cutomer age in years\n* '`Area Income`': Avg. Income of geographical area of consumer\n* '`Daily Internet Usage`': Avg. minutes a day consumer is on the internet\n* '`Ad Topic Line`': Headline of the advertisement\n* '`City`': City of consumer\n* '`Male`': Whether or not consumer was male\n* '`Country`': Country of consumer\n* '`Timestamp`': Time at which consumer clicked on Ad or closed window\n* '`Clicked on Ad`': 0 or 1 indicated clicking on Ad"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style('whitegrid')\nplt.style.use(\"fivethirtyeight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Get the Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/advertising/advertising.csv\")\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Exploratory Data Analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10, 8))\ndata.Age.hist(bins=data.Age.nunique())\nplt.xlabel('Age')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data[\"Area Income\"], data.Age)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data[\"Daily Time Spent on Site\"], data.Age, kind='kde')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.jointplot(data[\"Daily Time Spent on Site\"], data[\"Daily Internet Usage\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.pairplot(data, hue='Clicked on Ad')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Theory Behind Logistic Regression\n\nLogistic regression is the go-to linear classification algorithm for two-class problems. It is easy to implement, easy to understand and gets great results on a wide variety of problems, even when the expectations the method has for your data are violated.\n\n### Description\n\n#### Logistic Regression\n\nLogistic regression is named for the function used at the core of the method, the [logistic function](https://en.wikipedia.org/wiki/Logistic_function).\n\nThe logistic function, also called the **`Sigmoid function`** was developed by statisticians to describe properties of population growth in ecology, rising quickly and maxing out at the carrying capacity of the environment. Itâ€™s an S-shaped curve that can take any real-valued number and map it into a value between 0 and 1, but never exactly at those limits.\n\n$$\\frac{1}{1 + e^{-x}}$$\n\n$e$ is the base of the natural logarithms and $x$ is value that you want to transform via the logistic function."},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.linspace(-6, 6, num=1000)\nplt.figure(figsize=(10, 6))\nplt.plot(x, (1 / (1 + np.exp(-x))))\nplt.title(\"Sigmoid Function\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The logistic regression equation has a very similar representation like linear regression. The difference is that the output value being modelled is binary in nature.\n\n$$\\hat{y}=\\frac{e^{\\beta_0+\\beta_1x_1}}{1+\\beta_0+\\beta_1x_1}$$\n\nor\n\n$$\\hat{y}=\\frac{1.0}{1.0+e^{-\\beta_0-\\beta_1x_1}}$$\n\n$\\beta_0$ is the intecept term\n\n$\\beta_1$ is the coefficient for $x_1$\n\n$\\hat{y}$ is the predicted output with real value between 0 and 1. To convert this to binary output of 0 or 1, this would either need to be rounded to an integer value or a cutoff point be provided to specify the class segregation point.\n***\n### Learning the Logistic Regression Model\n\nThe coefficients (Beta values b) of the logistic regression algorithm must be estimated from your training data. This is done using [maximum-likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n\nMaximum-likelihood estimation is a common learning algorithm used by a variety of machine learning algorithms, although it does make assumptions about the distribution of your data (more on this when we talk about preparing your data).\n\nThe best coefficients would result in a model that would predict a value very close to 1 (e.g. male) for the default class and a value very close to 0 (e.g. female) for the other class. The intuition for maximum-likelihood for logistic regression is that a search procedure seeks values for the coefficients (Beta values) that minimize the error in the probabilities predicted by the model to those in the data (e.g. probability of 1 if the data is the primary class).\n\nWe are not going to go into the math of maximum likelihood. It is enough to say that a minimization algorithm is used to optimize the best values for the coefficients for your training data. This is often implemented in practice using efficient numerical optimization algorithm (like the Quasi-newton method).\n\nWhen you are learning logistic, you can implement it yourself from scratch using the much simpler gradient descent algorithm."},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(true, pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    from sklearn.metrics import precision_score, recall_score, f1_score\n\n    precision = precision_score(y_test, y_pred)\n    recall = recall_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred)\n    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data[\"Country\"] = data.Country.astype('category').cat.codes\ndata[\"City\"] = data.City.astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X = data.drop(['Timestamp', 'Clicked on Ad', 'Ad Topic Line'], axis=1)\ny = data['Clicked on Ad']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Prepare Data for Logistic Regression\nThe assumptions made by logistic regression about the distribution and relationships in your data are much the same as the assumptions made in linear regression.\n\nMuch study has gone into defining these assumptions and precise probabilistic and statistical language is used. My advice is to use these as guidelines or rules of thumb and experiment with different data preparation schemes.\n\nUltimately in predictive modeling machine learning projects you are laser focused on making accurate predictions rather than interpreting the results. As such, you can break some assumptions as long as the model is robust and performs well.\n\n- **Binary Output Variable:** This might be obvious as we have already mentioned it, but logistic regression is intended for binary (two-class) classification problems. It will predict the probability of an instance belonging to the default class, which can be snapped into a 0 or 1 classification.\n- **Remove Noise:** Logistic regression assumes no error in the output variable (y), consider removing outliers and possibly misclassified instances from your training data.\n- **Gaussian Distribution:** Logistic regression is a linear algorithm (with a non-linear transform on output). It does assume a linear relationship between the input variables with the output. Data transforms of your input variables that better expose this linear relationship can result in a more accurate model. For example, you can use log, root, Box-Cox and other univariate transforms to better expose this relationship.\n- **Remove Correlated Inputs:** Like linear regression, the model can overfit if you have multiple highly-correlated inputs. Consider calculating the pairwise correlations between all inputs and removing highly correlated inputs.\n- **Fail to Converge:** It is possible for the expected likelihood estimation process that learns the coefficients to fail to converge. This can happen if there are many highly correlated inputs in your data or the data is very sparse (e.g. lots of zeros in your input data)."},{"metadata":{},"cell_type":"markdown","source":"# 4. Implimenting Logistic Regression in Scikit-Learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(solver='liblinear', penalty='l1')\nlog_reg.fit(X_train, y_train)\n\ny_pred = log_reg.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Performance Measurement\n\n#### 1. Confusion Matrix\n- Each row: actual class\n- Each column: predicted class\n\nFirst row: Non-clicked Ads, the negative class:\n* 143 were correctly classified as Non-clicked Ads. **True negatives**. \n* Remaining 8 were wrongly classified as clicked Ads. **False positive**\n\n\nSecond row: The clicked Ads, the positive class:\n* 3 were incorrectly classified as Non-clicked Ads. **False negatives**\n* 146 were correctly classified clicked Ads. **True positives**\n\n#### 2. Precision\n\n**Precision** measures the accuracy of positive predictions. Also called the `precision` of the classifier ==> `97.98%`\n\n$$\\textrm{precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n\n#### 3. Recall\n\n`Precision` is typically used with `recall` (`Sensitivity` or `True Positive Rate`). The ratio of positive instances that are correctly detected by the classifier.\n\n$$\\textrm{recall} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$ ==> `94.80%`\n\n#### 4. F1 Score\n\n$F_1$ score is the harmonic mean of precision and recall. Regular mean gives equal weight to all values. Harmonic mean gives more weight to low values.\n\n\n$$F_1=\\frac{2}{\\frac{1}{\\textrm{precision}}+\\frac{1}{\\textrm{recall}}}=2\\times \\frac{\\textrm{precision}\\times \\textrm{recall}}{\\textrm{precision}+ \\textrm{recall}}=\\frac{TP}{TP+\\frac{FN+FP}{2}}$$ ==> `96.36%`\n\nThe $F_1$ score favours classifiers that have similar precision and recall.\n\n#### 5. Precision / Recall Tradeoff\n\nIncreasing precision reduced recall and vice versa"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\n\nprecisions, recalls, thresholds = precision_recall_curve(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim(0, 1.5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use(\"fivethirtyeight\")\nplt.figure(figsize=(12, 8))\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this chart, you can select the threshold value that gives you the best precision/recall tradeoff for your task.\n\nSome tasks may call for higher precision (accuracy of positive predictions). Like designing a classifier that picks up adult contents to protect kids. This will require the classifier to set a high bar to allow any contents to be consumed by children.\n\nSome tasks may call for higher recall (ratio of positive instances that are correctly detected by the classifier). Such as detecting shoplifters/intruders on surveillance images - Anything that remotely resemble \"positive\" instances to be picked up.\n\n***"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nplt.plot(precisions, recalls)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.title(\"PR Curve: precisions/recalls tradeoff\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## The Receiver Operating Characteristics (ROC) Curve\n\nInstead of plotting precision versus recall, the ROC curve plots the `true positive rate` (another name for recall) against the `false positive rate`. The `false positive rate` (FPR) is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the `true negative rate`, which is the ratio of negative instances that are correctly classified as negative.\n\nThe TNR is also called `specificity`. Hence the ROC curve plots `sensitivity` (recall) versus `1 - specificity`."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], \"k--\")\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,8)); \nplot_roc_curve(fpr, tpr)\nplt.show();","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nroc_auc_score(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Use PR curve whenever the **positive class is rare** or when you care more about the false positives than the false negatives\n\nUse ROC curve whenever the **negative class is rare** or when you care more about the false negatives than the false positives\n\n\nIn the example above, the ROC curve seemed to suggest that the classifier is good. However, when you look at the PR curve, you can see that there are room for improvement."},{"metadata":{},"cell_type":"markdown","source":"# 6. Logistic Regression Hyperparameter tuning"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\npenalty = ['l1', 'l2']\nC = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\nclass_weight = [{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}]\nsolver = ['liblinear', 'saga']\n\nparam_grid = dict(penalty=penalty, C=C, class_weight=class_weight, solver=solver)\n\ngrid = GridSearchCV(estimator=log_reg, param_grid=param_grid, scoring='roc_auc',\n                    verbose=1, n_jobs=-1, cv=10, iid=True)\ngrid_result = grid.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Predictions and Evaluations"},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = grid_result.predict(X_test)\n\nevaluate(y_test, y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 7. Summary\nIn this Notebook you discovered the logistic regression algorithm for machine learning and predictive modeling. You covered a lot of ground and learned:\n\n- What the logistic function is and how it is used in logistic regression.\n- That the key representation in logistic regression are the coefficients, just like linear regression.\n- That the coefficients in logistic regression are estimated using a process called maximum-likelihood estimation.\n- That making predictions using logistic regression is so easy that you can do it in excel.\n- That the data preparation for logistic regression is much like linear regression.\n- How to evaluate a machine learning classification problem.\n- How to tune logistic regression hyperparameters.\n\n## References:\n- [Scikit Learn Library](https://scikit-learn.org/stable/supervised_learning.html#supervised-learning)\n- [Logistic Regression for Machine Learning by Jason Brownlee PhD](https://machinelearningmastery.com/logistic-regression-for-machine-learning/)"}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}